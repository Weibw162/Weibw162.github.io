---
title: Kettle使用案例
date: 2022-03-02 15:44:19
permalink: /pages/60b5e8/
categories:
  - 其他技术
  - Kettle
  - Kettle基础
tags:
  - 
---



## 一、转换案例

案例一：把stu1的数据按id同步到stu2，stu2有相同id则更新数据

(1)在mysql中创建两张表

```sql
mysql> create database kettle;
mysql> use kettle;
mysql> create table stu1(id int,name varchar(20),age int);
mysql> create table stu2(id int,name varchar(20));
```

(2)往两张表中插入一些数据

```sql
mysql> insert into stu1 values(1001,'zhangsan',20),(1002,'lisi',18), (1003,'wangwu',23);
mysql> insert into stu2 values(1001,'wukong');
```

(3)在kettle中新建转换

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.2flbqf8b9728.webp)

(4)分别在输入和输出中拉出表输入和插入/更新

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.1vblxj8irnds.webp)

(5)双击表输入对象，填写相关配置，测试是否成功

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.2jwp9z5jr820.webp)

(6)双击 更新/插入对象，填写相关配置

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.2og5hhvkgwo0.webp)

(7)保存转换，启动运行，去mysql表查看结果

::: warning 

如果需要连接mysql数据库，需要先将mysql的连接驱动包复制到kettle的根目录下的lib目录中，否则会报错找不到驱动。

:::

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.678jshz7z0g0.webp)

## 二、作业案例

案例二：使用作业执行上述转换，并且额外在表stu2中添加一条数据，整个作业运行成功的话发邮件提醒 

(1)新建一个作业

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.1012yytpi0bk.webp)

(2) 按图示拉取组件

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.6piz87vzrv40.webp)

(3)双击Start编辑Start

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.3enqdw4f9c60.webp)

(4)双击转换，选择案例1保存的文件

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.5jy86hxpopc0.webp)

(5)双击SQL，编辑SQL语句

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.387r29jggbu0.webp)

(6)双击发送邮件，编辑发送邮件的设置信息

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.6o595hwyy700.webp)

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.2hysjc6kft60.webp)

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.4nr0dmkw8oe0.webp)

(7)保存作业并执行，然后去mysql查看结果和邮件信息

## 三、Hive-HDFS案例

案例三：将hive表的数据输出到hdfs

(1)因为涉及到hive和hbase的读写，需要先修改相关配置文件。

修改kettle安装目录下的data-integration\plugins\pentaho-big-data-plugin下的plugin.properties，设置active.hadoop.configuration=hdp26，并将如下配置文件拷贝到data-integration\plugins\pentaho-big-data-plugin\hadoop-configurations\hdp26下

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.1mvnfh0ixr5s.webp)

(2)启动hdfs，yarn，zookeeper，hbase集群的所有进程，启动hiveserver2服务

```shell
[atguigu@hadoop102 ~]$ hadoop.sh start    //自己写的hadoop启动脚本
[atguigu@hadoop102 ~]$ zk.sh start 		//自己写的zookeeper启动脚本
[atguigu@hadoop102 ~]$ /opt/module/hbase-1.3.1/bin/start-hbase.sh
[atguigu@hadoop102 ~]$ /opt/module/hive/bin/hiveserver2
```

(3)进入beeline，查看10000端口开启情况

```shell
[atguigu@hadoop102 ~]$ /opt/module/hive/bin/beeline
Beeline version 1.2.1 by Apache Hive
beeline> !connect jdbc:hive2://hadoop102:10000（回车）
Connecting to jdbc:hive2://hadoop102:10000
Enter username for jdbc:hive2://hadoop102:10000: atguigu（输入atguigu）
Enter password for jdbc:hive2://hadoop102:10000:（直接回车）
Connected to: Apache Hive (version 1.2.1)
Driver: Hive JDBC (version 1.2.1)
Transaction isolation: TRANSACTION_REPEATABLE_READ
0: jdbc:hive2://hadoop102:10000>（到了这里说明成功开启10000端口）
```

(4)创建两张表dept和emp

```sql
CREATE TABLE dept(deptno int, dname string,loc string)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';

CREATE TABLE emp(
empno int,
ename string,
job string,
mgr int,
hiredate string,
sal double,
comm int,
deptno int)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';
```

(5)插入数据

```sql
insert into dept values(10,'accounting','NEW YORK'),(20,'RESEARCH','DALLAS'),(30,'SALES','CHICAGO'),(40,'OPERATIONS','BOSTON');

insert into emp values
(7369,'SMITH','CLERK',7902,'1980-12-17',800,NULL,20),
(7499,'ALLEN','SALESMAN',7698,'1980-12-17',1600,300,30),
(7521,'WARD','SALESMAN',7698,'1980-12-17',1250,500,30),
(7566,'JONES','MANAGER',7839,'1980-12-17',2975,NULL,20);
```

(6)按下图建立流程图

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.1zjmydbe7puo.webp)

(7)设置表输入，连接hive

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.71jj43a95bc0.webp)

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.nuthr3w0uts.webp)

(8)设置排序属性

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.5emt9x83z0w0.webp)

(9)设置连接属性

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.1z5y8o4qzl8g.webp)

(10)设置字段选择

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.5x7kwmh5k540.webp)

(11)设置文件输出，点击浏览按钮

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.7ebjyyzijik0.webp)

选择存储路径为HDFS，并且新建一个Hadoop Cluster连接

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.48w20153mxc0.webp)

按照集群配置输入对应的参数 

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.4ls0nab1aq00.webp)

选择创建好的大数据集群连接

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.4lebxl4e1n20.webp)

然后再依次设置文本文件输出控件的文件，内容，字段三个设置页，注意一定要获取字段

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.3lidvb97v2g0.webp)

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.5rz0spxbylo0.webp)

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.42aut3x51oa0.webp)

(12)保存并运行转换，然后查看hdfs

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.6x34sglgl9w0.webp)

## 四、HDFS-Hbase案例

案例四：读取hdfs文件并将sal大于1000的数据保存到hbase中

(1) 在HBase中创建一张表用于存放数据

```shell
[atguigu@hadoop102 ~]$ /opt/module/hbase-1.3.1/bin/hbase shell
hbase(main):004:0> create 'people','info'
```

(2)按下图建立流程图

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.14y7w6qe2iak.webp)

(3)设置文件输入，连接hdfs

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.4ggi73nplfg0.webp)

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.4nzob8zzvgo0.webp)

注意千万别忘记获取字段，然后给字段设置格式

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.1f4a9cc42x5.webp)

(4)设置过滤记录

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.2r9n8bw2b9e.webp)

(5)设置HBase output控件

​	1.选择一个大数据集群连接

​	2.选择上面复制过来的hbase-site.xml

​	3.去第二个配置页面创建映射，然后保存映射（选择rowkey字段）

​	4.再回第一个配置页选择Hbase表以及刚刚创建好的映射

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.nnb91bsqck.webp)

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.7ii7d9ychko0.webp)

**注意：若报错没有权限往hdfs写文件，在Spoon.bat中第119行添加参数**

**"-DHADOOP_USER_NAME=atguigu" "-Dfile.encoding=UTF-8"**
(6)保存转换并运行，然户去hbase里面查看数据

![image](https://cdn.jsdelivr.net/gh/Weibw162/image-hosting@dev/Kettle/image.jepsy892qg0.webp)